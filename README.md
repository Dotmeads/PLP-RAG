## ğŸš€ PLP RAG â€“ Pet Care Retrieval-Augmented Generation

RAG system for pet care Q&A with hybrid retrieval, reranking, and LLM/baseline answer generation. Includes a Streamlit app and a simple CLI test runner.

### Key Components
- **Retrieval**: BM25 + dense embeddings (allâ€‘MiniLMâ€‘L6â€‘v2) fused via RRF
- **Reranking**: Crossâ€‘encoder MiniLM on topâ€‘k results
- **Answer Generation**: `free_llm_generator.py` with providers (Groq, DeepSeek) and a basic fallback
- **Vector Store**: ChromaDB via LangChain

### Current Repo Highlights
- Dedup ingestion: PDF files are skipped when a sameâ€‘name `.txt` exists
- Chunking tuned for markdown: larger chunks with overlap
- Batch add to Chroma to avoid â€œbatch size exceeds maximumâ€
- Simple test runner `test_questions.py` to try questions quickly

---

## ğŸ“¦ Setup

### 1) Create and activate venv
```bash
cd "/Users/dotsnoise/PLP RAG"
python3 -m venv .venv
source .venv/bin/activate
```

### 2) Install dependencies
Use the pinned versions in `requirements.txt` (these were validated together):
```bash
pip install -r requirements.txt
```

If you hit transformer/Hub compatibility issues, ensure versions align with:
- numpy 1.24.x, torch 2.1.x, transformers 4.36.x, sentence-transformers 2.2.2, huggingface_hub 0.19.4

### 3) Environment variables (recommended)
Do not commit API keys. Export them locally:
```bash
export GROQ_API_KEY="..."
export DEEPSEEK_API_KEY="..."
```

The system automatically prefers Groq â†’ DeepSeek â†’ basic fallback.

---

## â–¶ï¸ Usage

### A) Streamlit app
```bash
source .venv/bin/activate
streamlit run proposed_app.py --server.port 8501
```

### B) Python API
```python
from proposed_rag_system import ProposedRAGManager

rag = ProposedRAGManager(collection_name="proposed_rag_documents", use_openai=False)
rag.add_directory("documents")
result = rag.ask("What can I feed my cat?")
print(result["answer"])   # final answer
print(result["confidence"])  # float 0..1
print(result["sources"])  # list of source dicts
```

### C) Simple test runner
Edit questions in `test_questions.py`:
```python
questions = [
    "What can I feed my dog?",
    "What vaccines does my kitten need?",
]
```
Run it:
```bash
source .venv/bin/activate
python test_questions.py
```

---

## ğŸ§  How It Works

Highâ€‘level flow:
```
User Question
  â†’ BM25 and Dense Retrieval
  â†’ RRF Fusion of results
  â†’ Crossâ€‘encoder Reranking
  â†’ FreeLLM/Basic generation with citations
  â†’ Final Answer
```

### Retrieval
- `bm25_retriever.py`: BM25 with NLTK tokenization and stopwords
- `vector_store.py`: LangChain + Chroma vector store (allâ€‘MiniLMâ€‘L6â€‘v2)
- `rrf_fusion.py`: Reciprocal Rank Fusion to merge BM25 + dense

### Reranking
- `cross_encoder_reranker.py`: `cross-encoder/ms-marco-MiniLM-L-6-v2` reranks top results, thresholded

### Generation
- `free_llm_generator.py`: providers (Groq, DeepSeek, Hugging Face) with robust fallback to a basic extractive summary

### Ingestion
- `proposed_rag_system.py` â†’ `ingest_directory` collects supported files and skips `.pdf` when sameâ€‘name `.txt` exists
- `document_processor.py` uses a markdownâ€‘friendly splitter and larger chunk sizes with overlap

---

## ğŸ“ Repository Structure
```
bm25_retriever.py
cross_encoder_reranker.py
document_processor.py
free_llm_generator.py
proposed_app.py
proposed_rag_system.py
rrf_fusion.py
test_questions.py
vector_store.py
documents/  # your corpus (txt, md, etc.)
```

Notes:
- Local artifacts like `.venv/` and `chroma_db/` are ignored and should not be committed
- PDF ingestion is supported but will be skipped if a `.txt` with the same basename exists

---

## âš™ï¸ Configuration

`config.py` and inâ€‘code defaults control:
- Chunk size/overlap
- RRF fusion parameter `k`
- Reranker threshold and `max_rerank`

You can pass parameters via `ProposedRAGManager.ask(question, use_reranking=True, rerank_threshold=0.1, max_rerank=20)`.

---

## ğŸ§ª Evaluation (optional)

For quick sanity checks, rely on:
- Confidence score in results
- Source list and answer preview in `test_questions.py`

BLEU/ROUGE are available but often underâ€‘reflect RAG answer utility (due to freeâ€‘form, contextual responses).

---

## ğŸ› ï¸ Troubleshooting

- Missing packages: activate venv then `pip install -r requirements.txt`
- SentenceTransformers import failures: ensure compatible `huggingface_hub==0.19.4`
- Chroma schema errors: delete `chroma_db/` to reâ€‘initialize
- Large batch errors: document adds are already batched in `vector_store.py`
- NLTK tokenizer warnings: install/download required data (e.g., `punkt`)

---

## ğŸ”’ Security & Git Hygiene

- Do not commit secrets. Keep `api_keys.py` out of git (`.gitignore`) or use environment variables
- If a secret was committed, rotate it and rewrite history before pushing

---

## ğŸ“œ License

Provided asâ€‘is for educational and practical RAG use cases.

---

**ğŸ¾ Built for clear, sourced pet care answers.**
